This test did the following:

1) I loaded the image ("image.jpg"). 

2) I start a CPU and a wall timer.

3) I then do the following expensive operations:
3.1) I convert the image into a grayscale image (gray.jpg)
3.2) I then do canny edge detection on the gray image (canny.jpg)
3.2.1) However you will notice this produces lots and lots of noise... and in robotics thats bad... so I apply a filtering of the image... which eventually leads to canny2 (canny2.jpg)
3.3) I take the gray image and I threshold it... i.e I take the grayscale image and make anything above a certain threshold white and anything below a certain threshold black (binary.jpg). This operation removes noise and little objects i.e objects with very similar pixel values.
3.4) I then erode the image which is basically like blur it... this helps remove more of the noise along the outline of the objects (eroded.jpg)
3.5) I then group the pixels into two groups and display them as gray and black (bg.jpg)
3.6) I then combine the eroded and the grouped pixels to get (markers.jpg)
3.7) This image is then fed into an algorithm called watershed segmentation. Which further segments the image. This is then done canny edge detection on and produces (canny2.jpg)

I am not an expert in image processing and so I dont fully understand why people do segmentation like this but thats what the watershed tutorial suggests. If you look at the read me for image_processing the tutorial I used should be there.

It should also be noted that I stop the timers before I display the images.

I also dont know why the wall clock is shorter than the CPU time. The library I use for the CPU time is: <ctime> ;the library I use for the wallclock time is: <chrono>;